{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUSwsotgzbJO",
        "outputId": "86543010-bcaf-4a24-8fd9-0adbcd88c176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "from smart_open import open\n",
        "# from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# import tensorflow as tf\n",
        "\n",
        "# from tensorflow import keras\n",
        "\n",
        "# from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "# from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input\n",
        "# from tensorflow.keras.layers import concatenate\n"
      ],
      "metadata": {
        "id": "L5Uin9NGzhrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_location = \"dev_data\"\n",
        "test_location = \"test_data\"\n",
        "train_location = \"train_data\"\n",
        "\n",
        "devData = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "testData = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "trainData = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "\n",
        "\n",
        "dataset = np.concatenate((devData, np.concatenate((testData, trainData))))\n",
        "\n",
        "gc.collect()\n",
        "max_num_words = 17\n",
        "# model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/MCA Dataset/Copy of GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "# stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "T4hE7xDs0ZM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getAudioData(patientID, location, textD):\n",
        "    fileName = \"/content/drive/My Drive/MCA Dataset/\" + str(location) + \"/\" + str(int(patientID)) + \"_processed.csv\"  # Adjust the file naming convention\n",
        "    data = pd.read_csv(fileName)  # No header as columns are now known\n",
        "\n",
        "    # If your CSV has headers, you might not need to use `.values` and instead work with DataFrame directly\n",
        "    data_values = data.values if hasattr(data, 'values') else data\n",
        "\n",
        "    # Apply the helper function (if it's still needed)\n",
        "    # data_processed = audioDataHelper(data_values)\n",
        "\n",
        "    sentenceDatas = []\n",
        "    for sentence in textD:\n",
        "        sentenceStartime = sentence[0]\n",
        "        sentenceEndTime = sentence[1]\n",
        "        startIndex = math.floor(sentenceStartime / 0.01)  # Assuming your data is still sampled at 100Hz\n",
        "        endIndex = math.ceil(sentenceEndTime / 0.01)\n",
        "\n",
        "        sentenceData = data_values[startIndex: endIndex]\n",
        "        if sentenceData.size == 0:\n",
        "            # print(f\"Skipping empty slice at index range: {startIndex}-{endIndex}\")\n",
        "            continue  # Skip the current segment and continue with the next\n",
        "\n",
        "\n",
        "        sentenceData = np.average(sentenceData, axis=0)  # Averaging over time\n",
        "        sentenceData = sentenceData.reshape(1, -1)  # Reshaping to ensure it's a 2D array\n",
        "        sentenceDatas.append(sentenceData)\n",
        "\n",
        "    sentenceDatas = np.array(sentenceDatas)\n",
        "\n",
        "    # If you need to reshape it into a specific form, do it here\n",
        "    sentenceDatas = np.reshape(sentenceDatas, (textD.shape[0], -1))\n",
        "    return sentenceDatas\n"
      ],
      "metadata": {
        "id": "eOM_sbhTE3KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkDataPointExistence(patientID, split):\n",
        "  for i in split:\n",
        "    if(patientID == i[0]):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def getData(patientID, location,textD):\n",
        "  # print(\"PatientID: \" + str(int(patientID)))\n",
        "  retData = [int(patientID)]\n",
        "  # textD = getTextData(patientID, location)\n",
        "  audioD = getAudioData(patientID, location, textD)\n",
        "  # videoD = getVideoData(str(int(patientID)), location, textD)\n",
        "  # patientD = np.concatenate((textD, audioD, videoD), axis = 1)\n",
        "  # print(\"Final Patient Data: \" + str(patientD.shape))\n",
        "  return audioD\n",
        "\n",
        "  # return textD,audioD,videoD\n",
        "\n",
        "# def getTextData(patientID, location):\n",
        "#   fileName = \"/content/drive/My Drive/MCA Dataset/\"+ str(location) + \"/\" + str(int(patientID)) + \"_TRANSCRIPT.csv\"\n",
        "#   file = np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))\n",
        "\n",
        "#   # Remove All Utterences By Ellie:\n",
        "#   for i in range(len(file)):\n",
        "#     if(file[i][2] != 'Participant'):\n",
        "#       np.delete(file, i)\n",
        "#       i-=1\n",
        "\n",
        "#   # Remove Speaker Columnn\n",
        "#   file = np.delete(file, 2, 1)\n",
        "\n",
        "#   # Convert Text Into Word Vectors:\n",
        "#   w2vs = np.zeros((1, max_num_words*300))\n",
        "#   for i in range(len(file)):\n",
        "#     sentence = file[i][2]\n",
        "#     w2v = returnWordToVec(sentence)\n",
        "#     w2vs = np.concatenate((w2vs, w2v), axis = 0)\n",
        "#   w2vs = np.delete(w2vs, 0, 0)\n",
        "\n",
        "#   # Delete Sentences and Replace With W2Vs\n",
        "#   file = np.delete(file, 2, 1)\n",
        "#   # print(file)\n",
        "#   file = np.concatenate((file, w2vs), axis = 1)\n",
        "#   return file\n",
        "\n",
        "# def remove_StopWords(sentence):\n",
        "#     filtered_sentence = []\n",
        "#     for w in sentence:\n",
        "#         if w not in stop_words:\n",
        "#             filtered_sentence.append(w)\n",
        "#     return filtered_sentence\n",
        "\n",
        "# def returnWordToVec(sentence):\n",
        "#   global max_num_words, stop_words, model\n",
        "#   sentence = str(sentence).split(\" \")\n",
        "#   sentence = remove_StopWords(sentence)\n",
        "#   index_word = 0\n",
        "#   wordMatrix = np.zeros(max_num_words*300)\n",
        "#   for j in range(min(max_num_words, len(sentence))):\n",
        "#     try:\n",
        "#       word = sentence[j]\n",
        "#       if(word[0] == '<'):\n",
        "#         if(word.find('>')!=-1):\n",
        "#           word = word[1:-1]\n",
        "#         else:\n",
        "#           word = word[1:]\n",
        "#       else:\n",
        "#         if(word.find('>')!=-1):\n",
        "#           word = word[0:-1]\n",
        "#       ss = np.array(model[word])\n",
        "#       wordMatrix[index_word*300:(index_word+1)*300] = ss\n",
        "#       index_word+=1\n",
        "#     except Exception as e:\n",
        "#       continue\n",
        "#   wordMatrix = np.array(wordMatrix.reshape(1,-1))\n",
        "#   return wordMatrix\n",
        "\n",
        "def audioDataHelper(X):\n",
        "    for i in range(X.shape[0]):\n",
        "        if(X[i,1] == 0):\n",
        "            X[i,0] = 0\n",
        "            for j in range(7):\n",
        "                X[i,j+1] = 0\n",
        "    X = np.array(X)\n",
        "    return X\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lnr8dYKi0cBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_train_final = 'audio_train_final.pkl'\n",
        "text_train_final = 'text_train_final.pkl'\n",
        "Ytrain_final = 'Ytrain.pkl'\n",
        "\n",
        "audio_test_final = 'audio_test_final.pkl'\n",
        "text_test_final = 'text_test_final.pkl'\n",
        "Ytest_final = 'Ytest.pkl'\n",
        "\n",
        "audio_dev_final = 'audio_dev_final.pkl'\n",
        "text_dev_final = 'text_dev_final.pkl'\n",
        "Ydev_final = 'Ydev.pkl'"
      ],
      "metadata": {
        "id": "P_m21Y1DDdT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load progress if file exists, else return an empty list\n",
        "import os\n",
        "def load_progress(file_name):\n",
        "    if os.path.exists(file_name):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    else:\n",
        "        return []"
      ],
      "metadata": {
        "id": "jA8DkF7f1Hk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_test = load_progress('vectors_train.pkl')"
      ],
      "metadata": {
        "id": "GTzKo4sPEPWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_train = load_progress(audio_train_final)\n",
        "text_train = load_progress(text_train_final)\n",
        "Ytrain = load_progress(Ytrain_final)\n",
        "\n",
        "# audio_test = load_progress(audio_test_final)\n",
        "# text_test = load_progress(text_test_final)\n",
        "# Ytest = load_progress(Ytest_final)\n",
        "\n",
        "\n",
        "# Check if lists are empty, then initialize them\n",
        "if not audio_train: audio_train = []\n",
        "# if not audio_test: audio_test = []\n",
        "if not text_train: text_train = []\n",
        "if not Ytrain: Ytrain = []\n",
        "# if not Ytest: Ytest = []\n",
        "# if not text_test: text_test = []\n",
        "\n"
      ],
      "metadata": {
        "id": "k822Jom41JfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio_dev = []\n",
        "# text_dev =[]\n",
        "# Ydev = []"
      ],
      "metadata": {
        "id": "74F0yi5kIjZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=0\n",
        "\n",
        "\n",
        "for datapoint in dataset:\n",
        "\n",
        "    if checkDataPointExistence(datapoint[0], trainData):\n",
        "        # Process test data\n",
        "        text = vector_test[a]\n",
        "        audio = getData(datapoint[0], train_location,text)\n",
        "        audio_train.append(audio)\n",
        "        text_train.append(text)\n",
        "        Ytrain.append(datapoint[1])\n",
        "        a+=1\n",
        "        print(a)\n",
        "\n",
        "# print(a)\n",
        "\n",
        "# # Save progress after processing test data\n",
        "with open(audio_train_final, 'wb') as f:\n",
        "    pickle.dump(audio_train, f)\n",
        "with open(text_train_final, 'wb') as f:\n",
        "    pickle.dump(text_train, f)\n",
        "with open(Ytrain_final, 'wb') as f:\n",
        "    pickle.dump(Ytrain, f)\n",
        "\n",
        "# print(text1 == text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXtZkLb_1SXT",
        "outputId": "eaad2ea2-b222-455e-ff25-8c1ffb22ac8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=0\n",
        "for datapoint in dataset:\n",
        "\n",
        "\n",
        "    if checkDataPointExistence(datapoint[0], trainData):\n",
        "        # Process train data\n",
        "        text, audio = getData(datapoint[0], train_location)\n",
        "        audio_train.append(audio)\n",
        "        text_train.append(text)\n",
        "        Ytrain.append(datapoint[1])\n",
        "        a+=1\n",
        "\n",
        "print(a)\n",
        "\n",
        "# Save progress after processing train data\n",
        "with open(audio_train_final, 'wb') as f:\n",
        "    pickle.dump(audio_train, f)\n",
        "with open(text_train_final, 'wb') as f:\n",
        "    pickle.dump(text_train, f)\n",
        "with open(Ytrain_final, 'wb') as f:\n",
        "    pickle.dump(Ytrain, f)"
      ],
      "metadata": {
        "id": "fiaYBkZP1UVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the first pickle file\n",
        "import pickle\n",
        "with open('Ydev.pkl', 'rb') as file:\n",
        "    text_train = pickle.load(file)\n",
        "\n",
        "# Load the data from the second pickle file\n",
        "with open('Ytrain.pkl', 'rb') as file:\n",
        "    text_train2 = pickle.load(file)\n",
        "\n",
        "# Combine the data (This assumes that the data is list-like, adjust as necessary for your data structure)\n",
        "combined_data = text_train + text_train2\n",
        "\n",
        "# Save the combined data to a new pickle file\n",
        "with open('Ytrain_X.pkl', 'wb') as file:\n",
        "    pickle.dump(combined_data, file)\n",
        "\n",
        "print(\"Data combined and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Z5aHm3Mlos",
        "outputId": "335424a5-d1f8-4fe6-eb21-00e91901a014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio = load_progress('audio_train_finalX.pkl')\n",
        "text = load_progress('text_train_finalX.pkl')\n",
        "Y = load_progress('Ytrain_X.pkl')"
      ],
      "metadata": {
        "id": "e8erAna3NgRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(audio),len(text),len(Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npC1kITlQFwG",
        "outputId": "f33a71a7-0aaf-4a1a-c186-a32987a797f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142 142 142\n"
          ]
        }
      ]
    }
  ]
}
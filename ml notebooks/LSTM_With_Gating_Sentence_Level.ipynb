{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUSwsotgzbJO",
        "outputId": "293a24af-db38-4784-ba17-aff30b51694e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "from smart_open import open\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import concatenate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Uin9NGzhrQ",
        "outputId": "1e28a863-0cb8-4430-e4c6-e18745b18c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_location = \"dev_data\"\n",
        "test_location = \"test_data\"\n",
        "train_location = \"train_data\"\n",
        "\n",
        "devData = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "testData = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "trainData = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "\n",
        "\n",
        "dataset = np.concatenate((devData, np.concatenate((testData, trainData))))\n",
        "\n",
        "gc.collect()\n",
        "max_num_words = 17\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/MCA Dataset/Copy of GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "T4hE7xDs0ZM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sltc9FexMKoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eOM_sbhTE3KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTextData(patientID, location):\n",
        "  fileName = \"/content/drive/My Drive/MCA Dataset/\"+ str(location) + \"/\" + str(int(patientID)) + \"_TRANSCRIPT.csv\"\n",
        "  file = np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))\n",
        "\n",
        "\n",
        "\n",
        "  # Remove All Utterences By Ellie:\n",
        "  # for i in range(len(file)):\n",
        "  #   if(file[i][2] != 'Participant'):\n",
        "  #     np.delete(file, i)\n",
        "  #     i-=1\n",
        "\n",
        "  # Remove Speaker Columnn\n",
        "  file = np.delete(file, 2, 1)\n",
        "  # print(file[0],file[1])\n",
        "\n",
        "  # Convert Text Into Word Vectors:\n",
        "  w2vs = np.zeros((1, max_num_words*300))\n",
        "  for i in range(len(file)):\n",
        "    sentence = file[i][2]\n",
        "    w2v = returnWordToVec(sentence)\n",
        "    w2vs = np.concatenate((w2vs, w2v), axis = 0)\n",
        "  w2vs = np.delete(w2vs, 0, 0)\n",
        "\n",
        "  # Delete Sentences and Replace With W2Vs\n",
        "  file = np.delete(file, 2, 1)\n",
        "  # print(file)\n",
        "  # print(file)\n",
        "  file = np.concatenate((file,w2vs ), axis = 1)\n",
        "  # print(file)\n",
        "  return file\n",
        "\n",
        "def remove_StopWords(sentence):\n",
        "    filtered_sentence = []\n",
        "    for w in sentence:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence\n",
        "\n",
        "def returnWordToVec(sentence):\n",
        "  global max_num_words, stop_words, model\n",
        "  sentence = str(sentence).split(\" \")\n",
        "  sentence = remove_StopWords(sentence)\n",
        "  index_word = 0\n",
        "  wordMatrix = np.zeros(max_num_words*300)\n",
        "  for j in range(min(max_num_words, len(sentence))):\n",
        "    try:\n",
        "      word = sentence[j]\n",
        "      if(word[0] == '<'):\n",
        "        if(word.find('>')!=-1):\n",
        "          word = word[1:-1]\n",
        "        else:\n",
        "          word = word[1:]\n",
        "      else:\n",
        "        if(word.find('>')!=-1):\n",
        "          word = word[0:-1]\n",
        "      ss = np.array(model[word])\n",
        "      wordMatrix[index_word*300:(index_word+1)*300] = ss\n",
        "      index_word+=1\n",
        "    except Exception as e:\n",
        "      continue\n",
        "  wordMatrix = np.array(wordMatrix.reshape(1,-1))\n",
        "  return wordMatrix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lnr8dYKi0cBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_test =[]\n",
        "Ytest = []"
      ],
      "metadata": {
        "id": "_wVfvXxZ6OUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text= getData(datapoint[0], test_location)\n",
        "\n",
        "text_test.append(text)\n",
        "Ytest.append(datapoint[1])"
      ],
      "metadata": {
        "id": "LM_-uRk0ek8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def upsample(X_train,Y_train):\n",
        "  X_train_0 = X_train[Y_train==0]\n",
        "  X_train_1 = X_train[Y_train==1]\n",
        "\n",
        "  Y_train_1 = Y_train[Y_train==1]\n",
        "  # print(Y_train_1.shape)\n",
        "  # print(X_train_1.shape)\n",
        "  size = X_train_0.shape[0] - X_train_1.shape[0]\n",
        "  X = []\n",
        "  Y = []\n",
        "  X_train = list(X_train)\n",
        "  Y_train = list(Y_train)\n",
        "  while(size>0):\n",
        "    size -= 1\n",
        "    index = np.random.randint(0,X_train_1.shape[0]-1)\n",
        "    leave_index = np.random.randint(0,len(X_train)-1)\n",
        "    X_add = X_train_1[index]\n",
        "    X_leave = X_train[leave_index]\n",
        "\n",
        "    Y_add = Y_train_1[index]\n",
        "    Y_leave = Y_train[leave_index]\n",
        "\n",
        "    X_train[leave_index] = X_add\n",
        "    X_train.append(X_leave)\n",
        "\n",
        "    Y_train[leave_index] = Y_add\n",
        "    Y_train.append(Y_leave)\n",
        "\n",
        "\n",
        "  X_train = np.array(X_train)\n",
        "  Y_train = np.array(Y_train)\n",
        "  return X_train,Y_train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# audio_test = np.nan_to_num(audio_test)\n",
        "text_test = np.nan_to_num(text_test)\n",
        "\n",
        "\n",
        "for i in range(text_test.shape[0]):\n",
        "#   audio_test[i] = sklearn.preprocessing.normalize(audio_test[i])\n",
        "#   # video_test[i] = sklearn.preprocessing.normalize(video_test[i])\n",
        "  text_test[i] = sklearn.preprocessing.normalize(text_test[i])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2ZazEhOQyVL",
        "outputId": "2c33e444-726c-4e95-d71b-dfe8f6b9f580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 250, 5100)\n",
            "(200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Thresholding(Y_pred, threshold):\n",
        "  Y_pred2 = []\n",
        "  print(\"Y_pred: \", Y_pred.shape)\n",
        "  for i in range(len(Y_pred)):\n",
        "    if(Y_pred[i] < threshold):\n",
        "      Y_pred2.append(0)\n",
        "    else:\n",
        "      Y_pred2.append(1)\n",
        "\n",
        "  return np.array(Y_pred2)\n",
        "\n",
        "gc.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "651DvCL-Q9a_",
        "outputId": "d7f9c0a3-93c9-4124-a21f-adda6f85eb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text\n",
        "class Highway(layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Highway, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    n_sentences = input_shape[1]\n",
        "    n_features = input_shape[2]\n",
        "    carry_bias = keras.initializers.Constant(value=-1.0)\n",
        "    random_dist = keras.initializers.RandomNormal(mean=0.0, stddev=0.1)\n",
        "\n",
        "    carry_bias_2 = keras.initializers.Constant(value= 0.1)\n",
        "\n",
        "    self.W_T = self.add_weight(shape=(n_features, n_features),initializer = random_dist,trainable=True)\n",
        "    self.b_T = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias, trainable=True)\n",
        "\n",
        "    self.W = self.add_weight(shape=( n_features, n_features),initializer = random_dist, trainable=True)\n",
        "    self.b = self.add_weight(shape=( n_sentences, n_features),initializer = carry_bias_2, trainable=True)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    T = tf.sigmoid(tf.matmul(x, self.W_T) + self.b_T, name=\"transform_gate\")\n",
        "    H = tf.nn.relu(tf.matmul(x, self.W) + self.b, name=\"activation\")\n",
        "    C = tf.subtract(1.0, T, name=\"carry_gate\")\n",
        "\n",
        "    return tf.add(tf.multiply(H, T), tf.multiply(x, C), \"y\")\n",
        "\n",
        "\n",
        "\n",
        "#LSTM FOR TEXT\n",
        "# first input model\n",
        "class text:\n",
        "  input3 = Input(shape = (250,5100))\n",
        "  dense4 = Dense(1000)(input3)\n",
        "  dense5 = Dense(500)(dense4)\n",
        "  dense6 = Dense(250)(dense5)\n",
        "  # interpretation model\n",
        "  lstm = LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)(dense6)\n",
        "  output = Dense(1, activation='sigmoid')(lstm)\n",
        "  model = Model(inputs=input3, outputs=output)\n",
        "  # summarize layers\n",
        "  # print(model.summary())\n",
        "  # # plot graph\n",
        "  # print(plot_model(model, to_file='multiple_inputs.png'))\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate= 0.0001)\n",
        "  def run_model(self):\n",
        "    self.model.compile(optimizer=self.optimizer, loss='binary_crossentropy')\n",
        "    return self.model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EZJnildr2In",
        "outputId": "bb72e2b5-6f72-4d2f-8ab2-89566957accd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"-------------------------------------- TEXT (W/o GATING) SENTENCE LEVEL-------------------------------------------------\")\n",
        "# model1 = text()\n",
        "# model = model1.run_model()\n",
        "\n",
        "# model.fit(text_train,Ytrain, epochs=50,validation_split = 0.2, callbacks = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='min',\n",
        "#     baseline=None, restore_best_weights=True), batch_size = 137)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "pred = model.predict(text_test)\n",
        "\n",
        "# # print(classification_report(Ytest,Thresholding(pred,0.5)))\n",
        "# # print(classification_report(Ytest,Thresholding(pred,0.6)))\n",
        "# # print(classification_report(Ytest,Thresholding(pred,0.4)))\n",
        "# # print(classification_report(Ytest,Thresholding(pred,0.3)))\n",
        "# # print(classification_report(Ytest,Thresholding(pred,0.7)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSapnGvosA1C",
        "outputId": "0af297cb-825a-421e-a726-22ac2cf8b10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------- TEXT (W/o GATING) SENTENCE LEVEL-------------------------------------------------\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 16s 4s/step - loss: 0.6933 - val_loss: 0.6912\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6890 - val_loss: 0.6885\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6860 - val_loss: 0.6860\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6825 - val_loss: 0.6838\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6787 - val_loss: 0.6815\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6752 - val_loss: 0.6791\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6702 - val_loss: 0.6770\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 3s 1s/step - loss: 0.6662 - val_loss: 0.6753\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 4s 3s/step - loss: 0.6620 - val_loss: 0.6742\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6574 - val_loss: 0.6729\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6523 - val_loss: 0.6694\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6437 - val_loss: 0.6634\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6337 - val_loss: 0.6570\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.6220 - val_loss: 0.6510\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6118 - val_loss: 0.6456\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.6007 - val_loss: 0.6407\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.5890 - val_loss: 0.6351\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5772 - val_loss: 0.6283\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.5625 - val_loss: 0.6213\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.5485 - val_loss: 0.6146\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5335 - val_loss: 0.6079\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.5192 - val_loss: 0.6012\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.5059 - val_loss: 0.5939\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.4928 - val_loss: 0.5867\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 4s 3s/step - loss: 0.4805 - val_loss: 0.5808\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4709 - val_loss: 0.5734\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.4599 - val_loss: 0.5645\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.4490 - val_loss: 0.5553\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 4s 3s/step - loss: 0.4382 - val_loss: 0.5463\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4308 - val_loss: 0.5394\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.4218 - val_loss: 0.5349\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.4150 - val_loss: 0.5339\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4085 - val_loss: 0.5385\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.4038 - val_loss: 0.5459\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3977 - val_loss: 0.5484\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3938 - val_loss: 0.5470\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 4s 3s/step - loss: 0.3892 - val_loss: 0.5465\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3849 - val_loss: 0.5464\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3810 - val_loss: 0.5479\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3760 - val_loss: 0.5496\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3725 - val_loss: 0.5507\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3681 - val_loss: 0.5526\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3654 - val_loss: 0.5569\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3642 - val_loss: 0.5633\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3630 - val_loss: 0.5699\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.3605 - val_loss: 0.5703\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3591 - val_loss: 0.5678\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3577 - val_loss: 0.5666\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 3s 2s/step - loss: 0.3564 - val_loss: 0.5670\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 4s 2s/step - loss: 0.3553 - val_loss: 0.5701\n",
            "5/5 [==============================] - 1s 124ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the entire model to a HDF5 file.\n",
        "# model.save('LSTM_With_Gating_Sentence_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-jRH64ptg1d",
        "outputId": "374e6d44-5ce0-4a90-9081-cd1d1397e3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjeqJr4H_zWF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
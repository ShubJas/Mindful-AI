{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwM0SVe92ZCP",
        "outputId": "440bbf15-fdfa-488e-d2c9-c3da38c4566a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4PzmpQFky7x",
        "outputId": "fd432900-3b99-4ff4-c68c-ea0e62ddda46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|pip install pydub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn-xvgKe3m-W",
        "outputId": "5e0f45b8-7f76-4026-b1e3-6214d14ba67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UVFJeMg7Tj3",
        "outputId": "16fd849d-d7fa-43ef-9feb-c7d30b490ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file=\"/content/dilshad.wav\""
      ],
      "metadata": {
        "id": "Nl2V1BMo9LvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    # Initialize the recognizer\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    # Open and read the audio file\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        # Recognize the audio using the Google Web Speech API\n",
        "        transcribed_text = recognizer.recognize_google(audio_data)\n",
        "        return transcribed_text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Web Speech API could not understand the audio.\")\n",
        "        return \"\"\n",
        "    except sr.RequestError as e:\n",
        "        print(\"Could not request results from Google Web Speech API; {0}\".format(e))\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "lzxM8mNqTHhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from transformers import pipeline\n",
        "import sys\n",
        "model_name = \"sanskar/DepressionAnalysis\"\n",
        "classifier = pipeline(\"text-classification\", model=model_name)\n",
        "\n",
        "\n",
        "def predict_depression(audio_file):\n",
        "    \"\"\"\n",
        "    Function to make a prediction on the provided transcript.\n",
        "    \"\"\"\n",
        "    transcript= transcribe_audio(audio_file)\n",
        "\n",
        "    # Make a prediction\n",
        "    results = classifier(transcript)\n",
        "\n",
        "    # Extract the label and score from the results\n",
        "    label = results[0]['label']\n",
        "    score = results[0]['score']\n",
        "\n",
        "    # Return the results\n",
        "    return label, score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ArLIvRigkRKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Predict the depression level based on the transcript\n",
        "label, score = predict_depression(audio_file)\n",
        "\n",
        "# Output the prediction results\n",
        "print(f\"\\nPrediction: {label}\\nConfidence Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1pK09C8lTw2",
        "outputId": "528dbe6d-bd5f-4ada-a1d0-a58aca3b9476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction: Not Depressed\n",
            "Confidence Score: 0.9422476887702942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "model_path_json = 'depression_model_using_wav_audio.json'\n",
        "model_path = 'depression_model_using_wav_audio.h5'\n",
        "path_of_wav_audio_file='/content/dilshad.wav'\n",
        "path_of_scaler_filename='scaler_filename_dep.pkl'\n",
        "\n",
        "def emotion_model_using_wav_audio(path):\n",
        "  def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "  def stretch(data, rate=0.8):\n",
        "      return librosa.effects.time_stretch(y=data, rate=rate)\n",
        "\n",
        "\n",
        "  def shift(data):\n",
        "      shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "      return np.roll(data, shift_range)\n",
        "\n",
        "  def pitch(data, sampling_rate, n_steps=0.7):  # Changed pitch_factor to n_steps for clarity\n",
        "      return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=n_steps)\n",
        "\n",
        "\n",
        "  def extract_features(data,sample_rate):\n",
        "      # ZCR\n",
        "      result = np.array([])\n",
        "      zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "      result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "      # Chroma_stft\n",
        "      stft = np.abs(librosa.stft(data))\n",
        "      chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "      result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "      # MFCC\n",
        "      mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "      result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "      # Root Mean Square Value\n",
        "      rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "      result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "      # MelSpectogram\n",
        "      mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "      result = np.hstack((result, mel)) # stacking horizontally\n",
        "\n",
        "      return result\n",
        "\n",
        "  def get_features(path):\n",
        "      # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "      data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "      # without augmentation\n",
        "      res1 = extract_features(data,sample_rate)\n",
        "      result = np.array(res1)\n",
        "\n",
        "      # data with noise\n",
        "      noise_data = noise(data)\n",
        "      res2 = extract_features(noise_data,sample_rate)\n",
        "      result = np.vstack((result, res2)) # stacking vertically\n",
        "\n",
        "      # data with stretching and pitching\n",
        "      new_data= stretch(data)\n",
        "      data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "      res3 = extract_features(data_stretch_pitch,sample_rate)\n",
        "      result = np.vstack((result, res3)) # stacking vertically\n",
        "\n",
        "      return result\n",
        "\n",
        "  X= []\n",
        "  feature = get_features(path)\n",
        "  for ele in feature:\n",
        "      X.append(ele)\n",
        "  Features = pd.DataFrame(X)\n",
        "  X = Features.iloc[: ,:-1].values\n",
        "  scaler = joblib.load(path_of_scaler_filename)\n",
        "  x_test = scaler.transform(Features)\n",
        "  x_test = np.expand_dims(x_test, axis=2)\n",
        "  from keras.models import model_from_json\n",
        "  with open(model_path_json, \"r\") as json_file:\n",
        "      model_json = json_file.read()\n",
        "  model = model_from_json(model_json)\n",
        "  model.load_weights(model_path)\n",
        "\n",
        "  pred_test = model.predict(x_test)\n",
        "  Threshold=0.29\n",
        "  print(pred_test)\n",
        "  # if(pred_test>= Threshold):\n",
        "  #     return 1\n",
        "  # else:\n",
        "  #     return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "emotion_model_using_wav_audio(path_of_wav_audio_file)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SCR2SlpWl1AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R82GEab0QeG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
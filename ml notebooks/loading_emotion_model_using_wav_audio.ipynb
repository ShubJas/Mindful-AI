{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMm0sYT7WR_D",
        "outputId": "1f11524b-bbd6-48c3-f5e4-837a6ce23317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import librosa\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "model_path_json = 'depression_model_using_wav_audio.json'\n",
        "model_path = 'depression_model_using_wav_audio.h5'\n",
        "path_of_wav_audio_file='/content/dilshad.wav'\n",
        "path_of_scaler_filename='scaler_filename_dep.pkl'\n",
        "\n",
        "def emotion_model_using_wav_audio(path):\n",
        "  def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "  def stretch(data, rate=0.8):\n",
        "      return librosa.effects.time_stretch(y=data, rate=rate)\n",
        "\n",
        "\n",
        "  def shift(data):\n",
        "      shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "      return np.roll(data, shift_range)\n",
        "\n",
        "  def pitch(data, sampling_rate, n_steps=0.7):  # Changed pitch_factor to n_steps for clarity\n",
        "      return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=n_steps)\n",
        "\n",
        "\n",
        "  def extract_features(data,sample_rate):\n",
        "      # ZCR\n",
        "      result = np.array([])\n",
        "      zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "      result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "      # Chroma_stft\n",
        "      stft = np.abs(librosa.stft(data))\n",
        "      chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "      result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "      # MFCC\n",
        "      mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "      result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "      # Root Mean Square Value\n",
        "      rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "      result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "      # MelSpectogram\n",
        "      mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "      result = np.hstack((result, mel)) # stacking horizontally\n",
        "\n",
        "      return result\n",
        "\n",
        "  def get_features(path):\n",
        "      # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "      data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "      # without augmentation\n",
        "      res1 = extract_features(data,sample_rate)\n",
        "      result = np.array(res1)\n",
        "\n",
        "      # data with noise\n",
        "      noise_data = noise(data)\n",
        "      res2 = extract_features(noise_data,sample_rate)\n",
        "      result = np.vstack((result, res2)) # stacking vertically\n",
        "\n",
        "      # data with stretching and pitching\n",
        "      new_data= stretch(data)\n",
        "      data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "      res3 = extract_features(data_stretch_pitch,sample_rate)\n",
        "      result = np.vstack((result, res3)) # stacking vertically\n",
        "\n",
        "      return result\n",
        "\n",
        "  X= []\n",
        "  features = get_features(path)\n",
        "\n",
        "  for feature in features:\n",
        "      X.append(feature)\n",
        "  Features = pd.DataFrame(X)\n",
        "  scaler = joblib.load(path_of_scaler_filename)\n",
        "  x_test = scaler.transform(Features)\n",
        "  x_test = np.expand_dims(x_test, axis=2)\n",
        "\n",
        "  from keras.models import model_from_json\n",
        "  with open(model_path_json, \"r\") as json_file:\n",
        "      model_json = json_file.read()\n",
        "  model = model_from_json(model_json)\n",
        "  model.load_weights(model_path)\n",
        "\n",
        "  pred_test = model.predict(x_test)\n",
        "  Threshold=0.29\n",
        "  if(pred_test[0]>= Threshold):\n",
        "      return 1\n",
        "  else:\n",
        "      return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "emotion_model_using_wav_audio(path_of_wav_audio_file)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJCMXtjUWbvU",
        "outputId": "68d025cc-37c2-4d46-99b5-594c168cfc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 130ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-u82FF2hXEV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}